import numpy as np
import os 
from data_loader import load_data_2_dicts


# load the data
words_mapping, unigram_data, bigram_data, trigram_data = load_data_2_dicts(path = 'data')

def markov_model_2nd_order(past_1, past_2):
    """Simulates the 2nd order markov model. The current state of markov model depends upon the last two states.
    This function also incorporates back-offing
    Args:
        past1 (int): state at t - 1
        past2 (int): state at t - 2

    return:
        present_state:
    """
    if past_1 in trigram_data[past_2].keys(): # no need for back-off
        possible_states = np.array(list(trigram_data[past_2][past_1].keys()))
        prob_possible_states = np.array(list(trigram_data[past_2][past_1].values()))
        source = "trigram"

    else: # need for back-off
        if past_1 in bigram_data.keys():
            possible_states = np.array(list(bigram_data[past_1].keys()))
            prob_possible_states = np.array(list(bigram_data[past_1].values()))
            source = 'bigram'
        else:
            possible_states = np.array(list(unigram_data.keys()))
            prob_possible_states = np.array(list(unigram_data.values()))
            source = 'unigram'

    # convert the probabilites form log10 scale to decimal scale
    prob_possible_states = np.power(10, prob_possible_states)
    
    # normalize the probability
    prob_possible_states = prob_possible_states / np.sum(prob_possible_states)

    # find the present state
    present_state = np.random.choice(possible_states, p = prob_possible_states)

    return present_state, source

def generate_sequence():
    """_summary_
    """
    # first element is always <s>
    sequence = []
    sequence_source = []
    for key, value in words_mapping.items():
        if value == '<s>':
            break
    sequence.append(key)
    sequence_source.append('predefined')

    # for the 2nd word, use bigram data
    possible_states = np.array(list(bigram_data[key].keys()))
    prob_possible_states = np.array(list(bigram_data[key].values()))
    prob_possible_states = np.power(10, prob_possible_states)
    prob_possible_states = prob_possible_states / np.sum(prob_possible_states)

    sequence.append(np.random.choice(possible_states, p = prob_possible_states))
    sequence_source.append('bigram')

    while True:
        seq, source = markov_model_2nd_order(sequence[-1], sequence[-2])
        sequence.append(seq)
        sequence_source.append(source)
        if words_mapping[sequence[-1]] == '</s>':
            break
    
    for i in range(len(sequence)):
        print(words_mapping[sequence[i]]+ f"({sequence_source[i]})" + " ", end= " ")
    print(" ")


if __name__ == "__main__":
    print("Sentences generated by 2nd Order Markov model\n")
    for i in range(5):
        print(i + 1, end="     ")
        generate_sequence()
        print("\n")
        

